{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Python Spark SQL basic example, master=local[*]) created by getOrCreate at <ipython-input-1-0c26c15c4eb5>:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d6ac380b37b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# create the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# do something to prove it works\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Python Spark SQL basic example, master=local[*]) created by getOrCreate at <ipython-input-1-0c26c15c4eb5>:4 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "#os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "\n",
    "import pyspark\n",
    "conf = pyspark.SparkConf()\n",
    "\n",
    "# point to mesos master or zookeeper entry (e.g., zk://10.10.10.10:2181/mesos)\n",
    "conf.setMaster(\"spark://spark-master:7077\")\n",
    "# point to spark binary package in HDFS or on local filesystem on all slave\n",
    "# nodes (e.g., file:///opt/spark/spark-2.2.0-bin-hadoop2.7.tgz)\n",
    "#conf.set(\"spark.executor.uri\", \"hdfs://10.122.193.209/spark/spark-2.2.0-bin-hadoop2.7.tgz\")\n",
    "# set other options as desired\n",
    "#conf.set(\"spark.executor.memory\", \"2g\")\n",
    "#conf.set(\"spark.core.connection.ack.wait.timeout\", \"1200\")\n",
    "\n",
    "# create the context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(100000000))\n",
    "rdd.sumApprox(3)\n",
    "\n",
    "#textFile = sc.textFile(\"README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#textFile = sc.textFile(\"README.md\")\n",
    "textFile = sc.textFile(\"/tmp/data/Kiosk-OCC_201709140800.csv\")\n",
    "#textFile = sc.textFile(\"s3://itk-redshift-staging/Kiosk-OCC_201709140800.csv\")\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.19.0.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PemiSpart</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7efc58043908>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"PemiSpark\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/data/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark#.getOrCreate()\n",
    "spark.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.textFile(\"/tmp/data/Kiosk-OCC_201709140800.csv\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is an existing SparkSession\n",
    "df = spark.read.json(\"/tmp/data/people.json\")\n",
    "# Displays the content of the DataFrame to stdout\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  1|  one|\n",
      "|  2|  two|\n",
      "|  3|three|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   name\n",
       "0   1    one\n",
       "1   2    two\n",
       "2   3  three"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd_df = pd.DataFrame({\n",
    "    'id': [1,2,3],\n",
    "    'name': ['one', 'two', 'three']\n",
    "})\n",
    "\n",
    "#spark_df = context.createDataFrame(pandas_df)\n",
    "spark_df = spark.createDataFrame(pd_df)\n",
    "spark_df.show()\n",
    "\n",
    "spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------+\n",
      "|beer_id|   sold_at|quantity|\n",
      "+-------+----------+--------+\n",
      "|      1|2017-01-01|       3|\n",
      "|      2|2017-01-02|       3|\n",
      "|      3|2017-01-03|       5|\n",
      "|      4|2017-01-04|       8|\n",
      "|      5|2017-01-04|       6|\n",
      "|      1|2017-01-06|       1|\n",
      "+-------+----------+--------+\n",
      "\n",
      "+---+-------------+-----+--------------------+--------------------+\n",
      "| id|         name|style|                 abv|               price|\n",
      "+---+-------------+-----+--------------------+--------------------+\n",
      "|  1|     SpinCyle|  IPA|51.82000000000000...|2.810000000000000000|\n",
      "|  2|     OldStyle| Pale|22.23000000000000...|97.40000000000000...|\n",
      "|  3|   Pipewrench|  IPA|60.29000000000000...|14.80000000000000...|\n",
      "|  4|AbstRedRibbon|Lager|43.30000000000000...|85.38000000000000...|\n",
      "+---+-------------+-----+--------------------+--------------------+\n",
      "\n",
      "+-------+-------------+-----+----------+--------+----------+----------+\n",
      "|beer_id|         name|style|   sold_at|quantity|unit_price|sell_price|\n",
      "+-------+-------------+-----+----------+--------+----------+----------+\n",
      "|      5|         null| null|2017-01-04|       6|      null|      null|\n",
      "|      1|     SpinCyle|  IPA|2017-01-01|       3|      2.81|      8.43|\n",
      "|      1|     SpinCyle|  IPA|2017-01-06|       1|      2.81|      2.81|\n",
      "|      3|   Pipewrench|  IPA|2017-01-03|       5|     14.80|     74.00|\n",
      "|      2|     OldStyle| Pale|2017-01-02|       3|     97.40|    292.20|\n",
      "|      4|AbstRedRibbon|Lager|2017-01-04|       8|     85.38|    683.04|\n",
      "+-------+-------------+-----+----------+--------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pemi\n",
    "import pemi.data\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"PemiSpark\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/tmp/data/spark-warehouse\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "\n",
    "sales_schema = {\n",
    "    'beer_id':  {'ftype': 'integer', 'required': True},\n",
    "    'sold_at':  {'ftype': 'date', 'in_format': '%m/%d/%Y', 'required': True},\n",
    "    'quantity': {'ftype': 'integer', 'required': True}\n",
    "}\n",
    "\n",
    "beers_schema = {\n",
    "    'id':       {'ftype': 'integer', 'required': True},\n",
    "    'name':     {'ftype': 'string', 'required': True},\n",
    "    'style':    {'ftype': 'string'},\n",
    "    'abv':      {'ftype': 'float'},\n",
    "    'price':    {'ftype': 'decimal', 'precision': 16, 'scale': 2}\n",
    "}\n",
    " \n",
    "sales_table = pemi.data.Table(\n",
    "    '''\n",
    "    | beer_id | sold_at    | quantity |\n",
    "    | -       | -          | -        |\n",
    "    | 1       | 01/01/2017 | 3        |\n",
    "    | 2       | 01/02/2017 | 3        |\n",
    "    | 3       | 01/03/2017 | 5        |\n",
    "    | 4       | 01/04/2017 | 8        |\n",
    "    | 5       | 01/04/2017 | 6        |\n",
    "    | 1       | 01/06/2017 | 1        |\n",
    "    ''',\n",
    "    schema=sales_schema,\n",
    "    fake_with={\n",
    "        'beer_id': { 'valid': lambda: pemi.data.fake.random_int(1,4) },\n",
    "        'sold_at': { 'valid': lambda: pemi.data.fake.date_time_this_decade().date() },\n",
    "        'quantity': {'valid': lambda: pemi.data.fake.random_int(1,100) },\n",
    "        'bumpkin': { 'valid': lambda: pemi.data.fake.word(['bumpkin A', 'bumpkin B', 'bumpkin C']) }\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "beers_table = pemi.data.Table(\n",
    "    '''\n",
    "    | id | name          | style |\n",
    "    | -  | -             | -     |\n",
    "    | 1  | SpinCyle      | IPA   |\n",
    "    | 2  | OldStyle      | Pale  |\n",
    "    | 3  | Pipewrench    | IPA   |\n",
    "    | 4  | AbstRedRibbon | Lager |\n",
    "    ''',\n",
    "    schema=beers_schema,\n",
    "    fake_with={\n",
    "        'abv': {'valid': lambda: pemi.data.fake.pydecimal(2, 2, positive=True)},\n",
    "        'price': {'valid': lambda: pemi.data.fake.pydecimal(2, 2, positive=True)}\n",
    "    }\n",
    ")\n",
    "\n",
    "beers_df = spark.createDataFrame(beers_table.df)\n",
    "sales_df = spark.createDataFrame(sales_table.df)\n",
    "sales_df.show()\n",
    "beers_df.show()\n",
    "\n",
    "\n",
    "beers_df.createOrReplaceTempView('beers')\n",
    "sales_df.createOrReplaceTempView('sales')\n",
    "\n",
    "beer_sales_df = spark.sql('''\n",
    "    SELECT\n",
    "      sales.beer_id,\n",
    "      beers.name,\n",
    "      beers.style,\n",
    "      sales.sold_at,\n",
    "      sales.quantity,\n",
    "      CAST(beers.price as decimal(10, 2)) AS unit_price,\n",
    "      CAST(beers.price * sales.quantity AS DECIMAL(10,2)) AS sell_price\n",
    "    FROM\n",
    "      sales\n",
    "    LEFT JOIN\n",
    "      beers\n",
    "    ON\n",
    "      sales.beer_id = beers.id\n",
    "''')\n",
    "\n",
    "beer_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beer_id</th>\n",
       "      <th>name</th>\n",
       "      <th>style</th>\n",
       "      <th>sold_at</th>\n",
       "      <th>quantity</th>\n",
       "      <th>unit_price</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SpinCyle</td>\n",
       "      <td>IPA</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>3</td>\n",
       "      <td>99.780000000000000000</td>\n",
       "      <td>299.340000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>SpinCyle</td>\n",
       "      <td>IPA</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>99.780000000000000000</td>\n",
       "      <td>99.780000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Pipewrench</td>\n",
       "      <td>IPA</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>5</td>\n",
       "      <td>1.600000000000000000</td>\n",
       "      <td>8.000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>OldStyle</td>\n",
       "      <td>Pale</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>3</td>\n",
       "      <td>5.200000000000000000</td>\n",
       "      <td>15.600000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>AbstRedRibbon</td>\n",
       "      <td>Lager</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>8</td>\n",
       "      <td>51.620000000000000000</td>\n",
       "      <td>412.960000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beer_id           name  style     sold_at  quantity             unit_price  \\\n",
       "0        5           None   None  2017-01-04         6                   None   \n",
       "1        1       SpinCyle    IPA  2017-01-01         3  99.780000000000000000   \n",
       "2        1       SpinCyle    IPA  2017-01-06         1  99.780000000000000000   \n",
       "3        3     Pipewrench    IPA  2017-01-03         5   1.600000000000000000   \n",
       "4        2       OldStyle   Pale  2017-01-02         3   5.200000000000000000   \n",
       "5        4  AbstRedRibbon  Lager  2017-01-04         8  51.620000000000000000   \n",
       "\n",
       "               sell_price  \n",
       "0                    None  \n",
       "1  299.340000000000000000  \n",
       "2   99.780000000000000000  \n",
       "3    8.000000000000000000  \n",
       "4   15.600000000000000000  \n",
       "5  412.960000000000000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = beer_sales_df.toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     None\n",
       "1    99.780000000000000000\n",
       "2    99.780000000000000000\n",
       "3     1.600000000000000000\n",
       "4     5.200000000000000000\n",
       "5    51.620000000000000000\n",
       "Name: unit_price, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(pemi.field)\n",
    "importlib.reload(pemi)\n",
    "\n",
    "schema=pemi.Schema({\n",
    "    'beer_id':    {'ftype': 'integer', 'required': True},\n",
    "    'name':       {'ftype': 'string'},\n",
    "    'style':      {'ftype': 'string'},\n",
    "    'sold_at':    {'ftype': 'date', 'in_format': '%m/%d/%Y', 'required': True},\n",
    "    'quantity':   {'ftype': 'integer', 'required': True},\n",
    "    'unit_price': {'ftype': 'decimal', 'precision': 16, 'scale': 2},\n",
    "    'sell_price': {'ftype': 'decimal', 'precision': 16, 'scale': 2}\n",
    "})\n",
    "\n",
    "\n",
    "#df['sold_at'].apply(schema['sold_at'].in_converter)\n",
    "df['unit_price'].apply(schema['unit_price'].in_converter)\n",
    "#df['unit_price'].apply(lambda x: x.__class__.__name__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
